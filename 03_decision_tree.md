# 3. 의사결정나무 모형 (Decision Tree) **

[blog link](http://www.dodomira.com/2016/05/29/564/)

## 1) 의사결정나무

> 기계학습 중 하나로 특정 항목에 대한 의사결정 규칙을 나무형태로 분류해나가는 분석기법  

#### 장점  
- 분석과정이 직관적이고 이해하기 쉬움 (화이트박스 모델)
  - 반면 인공신경망의 경우, 결과에 대한 설명을 이해하기 어려운 대표적인 블랙박스 모델 
- 수치형/범주형 변수 모두 사용 가능  
- 계산 비용이 낮아 대규모의 데이터 셋에서도 빠르게 연산 가능  
- 선형성, 정규성, 등분상성 등 수학적 가정 불필요한 **비모수적 모형**
- 유용한 입력변수의 파악 
- 예측변수 간의 상호작용 및 비선형성 고려한 분석 
- 다양한 패키지 존재 - 상황에 맞게 비교해서 가장 좋은 예측력 보이는 패키지 선택 

#### 단점
- 분류 기준값의 경계선 부근의 자료값에 대해서 오차 큼
- 각 예측변수의 효과 파악 어려운 (로지스틱회귀랑 비슷)
- 새로운 자료에 대한 예측 불안정 - bootstrapping, bagging 등의 방법으로 사용 

#### 분석방법  
- 카이스퀘어, T검정, F검정 등 통계학에 기반한 CART 및 CHAID 알고리즘
- 엔트로피, 정보 이득 등 기계학습에 기반한 ID3, C4.5, C5.0 알고리즘
  
#### 구조  
- 뿌리마디: 시작되는 마디로 전체 자료를 포함
- 자식마디: 하나의 마디로부터 분리되어 나간 2개 이상의 마디들
- 부모마디: 주어진 마디의 상위 마디
- 최종마디: 자식마디가 없는 마디
- 중간마디: 부모마디와 자식마지가 모두 있는 마디
- 가지: 뿌리마디로부터 최종마디까지 연결된 마디들
- 깊이: 뿌리마디부터 최종마디까지의 중간마디들의 수 
  
#### R에서 패키지 비교 (TREE, RPART, PARTY 패키지)

> 의사결정나무를 만들 때 가지치기를 하는 방법에 차이가 존재 

- (1) TREE 
  - Binary Recursive Partitionaing
- (2) RPART 
  - CART (Classification and Regression Trees)  
  - 이 패키지들은 엔트로피, 지니계수를 기준으로 가지치기를 할 변수를 결정 
  - 상대적으로 속도를 빠르지만 과적합화의 위험성 존재  
  - 따라서 pruning의 과정을 거쳐서 의사결정나무를 최적화하는 과정 필요  
- (3) PARTY - Unbiased recursive partitionaing based on permutation tests 
  - p-test를 거친 significance를 기준으로 가지치기를 할 변수를 결정 
  - biased될 위험이 없어 별도로 pruning할 필요 없음 
  - 하지만 변수의 레벨이 31개까지로 제한 

## 2) 데이터 분할과 과대적합

모델을 만들 때는 보통 데이터를 training 과 test set으로 나누어 사용

#### 과대적합 
- 학습에 사용한 training이 test와 비슷하다면 앞에서 만든 모델의 정확도는 높게나오겠지만,
- 복잡한 모델을 만든다면 training set에만 정확한 모델이 나올 수 있음 
- 새로운 데이터가 입력되면 잘못된 결과 예측 


#### 과소적합
- 모델은 간단하지만 training set에서조차 정확한 결과가 나오지 않음


#### 해결방안
- 더 많은 데이터 확보 필요 
- 확보한 데이터로부터 더 다양한 특징(feature) 찾아서 학습에 사용 

## 3) 의사결정나무 구분

### (1) 분류나무 (classification tree)

- 목표변수가 이산형
- 상위 노드에서 가지분할을 수행할 때, 분류변수와 분류 기준값의 선택방법
  - 카이제곱통계량의 p값 : 값이 작을수록 이질성 큼 = 키우기!
  - 지니 지수 : 값이 클수록 이질성 큼 = 낮추기!
  - 엔트로피 지수 : 값이 클수록 이질성 큼 = 낮추기!
- **자식노드 내의 이질성이 큼을 의미** = 따라서 이질성이 가장 작아지는 방향으로 가지분할 수행 
  
### (2) 회귀나무 (regression tree)

- 목표변수가 연속형
- 분류변수와 분류 기준값 선택방법
  - F통계량의 p값 : 값이 클수록 이질성 큼 = 낮추기! 
    - p값이 크다는 건 오차의 변동에 비해 처리의 변동이 크다는 것을 의미
  - 분산의 감소량 : 값이 작을수록 이질성 큼 = 높이기!
 
#### + 정지규칙 

- 더 이상 분리가 일어나지 않고 현재의 마디가 최종마디가 되도록하는 여러 규칙
- 깊이, 자식마디의 최소 관측치 수, 카이제곱통계량, 지니 지수, 엔트로피 지수 등

#### + 가지치기

- 최종마디가 너무 많으면 = 모형이 과대적합될 가능성 높음 
- 분류된 관측치의 비율, MSE 등 고려하여 가지치기 규칙 제공 

### (rpart) 패키지 이용한 iris 데이터 의사결정나무 분석 

```{r}
library(rpart)
k = rpart(Species~., data=iris)
k
```

#### (rpart.plot()) 사용한 다양한 시각화 

```{r}
install.packages('rpart.plot')
library(rpart.plot)
prp(k, type=4, extra=2, digits=3)
```

#### (rpart) confusionMatri 함수 통한 정확성 평가

```{r}
head(predict(k, newdata=iris, type='class'))
printcp(k)
plotcp(k)
rpartpred = predict(k, iris, type='class')
confusionMatrix(rpartpred, iris$Species)
```
